---
layout: distill
title: MixNAR
description: A blogpost about neural algorithmic reasoning, modularity (mixture of experts) and how trying to implement the first modular reasoner gave insights about one of the most difficult algorithms to emulate. *TODO*
date: 2025-04-28
future: true
htmlwidgets: true
hidden: false

# Anonymize when submitting
# authors:
#   - name: Anonymous

authors:
  - name: Anonymous

#  - name: Albert Einstein
#    url: "https://en.wikipedia.org/wiki/Albert_Einstein"
#    affiliations:
#      name: IAS, Princeton
#  - name: Boris Podolsky
#    url: "https://en.wikipedia.org/wiki/Boris_Podolsky"
#    affiliations:
#      name: IAS, Princeton
#  - name: Nathan Rosen
#    url: "https://en.wikipedia.org/wiki/Nathan_Rosen"
#    affiliations:
#      name: IAS, Princeton

# must be the exact same name as your blogpost
bibliography: 2025-04-28-modnar.bib  

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly. 
#   - please use this format rather than manually creating a markdown table of contents.
toc:
  - name: Before we start
  - name: NAR-101
    subsections:
    - name: Alignment
    - name: The CLRS-30 benchmark
    - name: Current SOTAs
  - name: Knuth-Morris-Pratt
  - name: Citations
  - name: Footnotes
  - name: Code Blocks
  - name: Diagrams
  - name: Tweets
  - name: Layouts
  - name: Other Typography?

# styles block
_styles: >
  .takeaway {
    border: 1px solid #333; /* Dark grey border */
    border-radius: 12px; /* Rounded corners */
    padding: 15px;
    background-color: #f5f5f5; /* Light background to contrast with the border */
    font-style: italic
  }
  .takeaway h2 {
    margin-top: 0;
  }




---

## Before we start

To save the reader their time, I will begin my writings by making clear one
important thing. _This is a blogpost and it will read as such._ Therefore it
will not follow a traditional paper structure.<d-footnote> Starts with
introduction complemented with 3+ contributions, followed by related work,
methodology, experiments, conclusions.</d-footnote> Instead, I decided to lay
out my train of thought in a way that would be interesting from the perspective
of an NAR expert, but also from the perspective of an early career researcher
who would like to glimpse how a (self-proclaimed) NAR researcher thinks like.

However, I do understand that some might be newcomers to the topic. Therefore,
I would like to start this blogpost with...

## NAR-101

Consider how a programmer may solve a problem: They
would check what instruments they have in their toolkit -- books such
as _Introduction to Algorithms_ <d-cite key="clrs"></d-cite> (a.k.a. CLRS) or _Algorithms_ <d-cite
key="sedgewick2011algorithms"></d-cite>, choose the most suitable out of the
set and start matching the problem to the tool. Manually performing the last
part may involve rethinking the problem at hand, consulting their teammates or
_performing various forms of witchcraft_, such as plugging random pieces of
data or scalar functions of them as the input to the algorithm and seeing what
happens.  It is conjectured that this gap between theoretical algorithms and
their real-life execution largely stems from the _scalar bottleneck_ --
having to represent multitude of factors with a single scalar. <d-cite
key="narblueprint"></d-cite>

**N**eural **a**lgorithmic **r**easoning (**NAR**) focuses on training graph neural
networks (**GNNs**) to execute algorithms in a vectorial latent space<d-footnote>i.e.,
node features are 64-dimensional vectors</d-footnote> so we can overcome the
scalar bottleneck. When deploying an algorithm of interest to a problem we can
leave it to gradient descent instead of the programmer to find an appropriate
mapping. <d-cite key="deac2020xlvin"></d-cite><d-cite
key="tang2020towards"></d-cite><d-cite
key="he2022continuous"></d-cite><d-cite
key="numeroso2023dar"></d-cite><d-cite key="panagiotaki2024naricp"></d-cite>

### Alignment 

Even from the earliest papers on NAR <d-cite
key="velickovic2020neural"></d-cite> it became obvious that training a robust
neural reasoner, _that can give correct outputs for inputs of any (even larger
ones than trained on) sizes_ is not an easy task. It requires _architectural
alignment_. Giving a precise definition is a lengthy task for a blogpost, but
some intuition never hurts:


{% include figure.html path="assets/img/2025-04-28-modnar/alignment.png" class="alignment" %}
<div class="caption">
    Figure source: Veličković <d-cite key="velickovic2023nargradient"></d-cite>
</div>

<blockquote>
    Neural networks extrapolate well if the algorithm can be separated into subfunctions and each of them is easily learnable by a corresponding neural submodule.
    - The core idea of Xu et al. <d-cite key="xu2020howneural"></d-cite>
</blockquote>

Or in other words, GNNs extrapolate very easily on the Bellman-Ford algorithm,
because:
- the intermediate algorithm values correspond to the latent features of the
  nodes
- the multi-layer perceptron (**MLP**) that computes the messages can easily
  learn linear functions like addition
- we can choose the GNN neighbourhood aggregation to match the operator
  ($\min$) of the algorithm

This is just a simplified example and aligning neural models with other
algorithmic properties is an active (and tough) area of research. As I will
strive to keep the 101 just as much as you would need to understand **this**
blogpost, I refer the enthusiastic reader to another one <d-cite
key="velickovic2023nargradient"></d-cite> for more details and papers on NAR
alignment.

### The CLRS-30 Benchmark

The CLRS-30 benchmark <d-cite key="velivckovic2022clrs"></d-cite> includes more
than just 30 instances from the _Introduction to Algorithms_ textbook. It
provides a unifying framework for representing algorithmic problems to the
GNNs. Getting familiar with the API/pipeline requires some time to master, so
if you are looking for where to start from, [this EEML 2024 tutorial][eeml2024]
is a good place to start

[eeml2024]: https://github.com/eemlcommunity/PracticalSessions2024/blob/main/1_reasoning/Reasoning_tutorial_solution.ipynb

However, I believe having a good grasp of the CLRS-30 lingo is sufficient for
understanding **this** blogpost and does not require us studying all the
codebase. We can get to the required level of knowledge, again, through
examples.


To start with, at a given timestep, all information about an algorithm is
stored as a set of `(Stage, Location, Datatype)` triplets.

{% highlight python linenos %}

class Stage:
  INPUT = 'input'
  OUTPUT = 'output'
  HINT = 'hint'

class Location:
  NODE = 'node'
  EDGE = 'edge'
  GRAPH = 'graph'

class Type:
  SCALAR = 'scalar'
  CATEGORICAL = 'categorical' # for variables with categorical featues
  MASK = 'mask' # binary features
  MASK_ONE = 'mask_one' # same as MASK, just one element is allowed to be 1
  # Make no mistake -- mask_one is still a binary, not a categorical feature
  POINTER = 'pointer'
  # ... some types omitted for brevity ...

{% endhighlight %}

`Location` (notion for node/edge/graph features), as well as the `Stage.INPUT`
and `Stage.OUTPUT` are self-explanatory. `Stage.HINT` is CLRS-specific and it
allows for modelling the trajectory of the algorithm -- at any timestep $\tau$
the NAR would take as input hints computed from the previous step $\tau-1$ and
learn to predict what the hints are for step $\tau$. `Type` dictates how
features should be processed, including what losses should we use for training.
Of all types, the only non-trivial one is `Type.POINTER`. E.g. the node pointer
for node $i$ behaves like an edge (yes, it is not a typo) from $i$ to any other
node $j$, with the restriction that there could be only **one** pointer from
node $i$.<d-footnote>Some NAR development frameworks, such as the one I used
here, put an even harder restriction that a pointer must be an edge in the
graph. This is usually done to maintain low memory usage -- $O(E)$ instead of
$O(V^2)$. For this project, however, $E=V^2$, so we did not get the benefits...
but I have a preference for PyTorch over JAX</d-footnote> When (re)encoded as
an input/hint node pointers become edge features.

How all the data for an algorithm looks like is described via specifications,
like the one below, corresponding to the Bellman-Ford algorithm.<d-footnote>If
the names of the variables are too cryptic for your taste, a pro advice from me
is to check the _Introduction to Algorithms_. The authors of the CLRS benchmark
usually use the same variable names.</d-footnote>

{% highlight python linenos %}

SPECS = types.MappingProxyType({
    # ... many algorithms omitted for brevity ...
    'bellman_ford': {
        # just a tie-breaker, in case two solutions are identical
        'pos': (Stage.INPUT, Location.NODE, Type.SCALAR),
        # starting node
        's': (Stage.INPUT, Location.NODE, Type.MASK_ONE),
        # weight matrix (NxN)
        'A': (Stage.INPUT, Location.EDGE, Type.SCALAR),
        # adjacency matrix
        'adj': (Stage.INPUT, Location.EDGE, Type.MASK),
        # predecessors array pi[i] holds the id of the
        # tip of the pointer from i
        'pi': (Stage.OUTPUT, Location.NODE, Type.POINTER),
        # same as above, but for each timestep of algorithm's execution;
        # the default starting pi array consists of self-loops (pi[i]=i) 
        'pi_h': (Stage.HINT, Location.NODE, Type.POINTER),
        # shortest distances at a given timestep
        'd': (Stage.HINT, Location.NODE, Type.SCALAR),
        # reachable nodes at a given timestep
        'msk': (Stage.HINT, Location.NODE, Type.MASK),
    },
    # ... many algorithms omitted for brevity ...
})

{% endhighlight %}


### Current SOTAs

As you can see from the year of publication, this benchmark has been around for
a while. A lot of research has went into creating models that improve on
previous ones and so on and so forth. As a result, only for all but three (out
of thirty) algorithms no one has reported OOD accuracy above 80%. <d-cite
key="xu2024recurrent"></d-cite> The most challenging of the trio and the one
I tried to tackle is...


# Knuth-Morris-Pratt

## Encountering it
{% include figure.html path="assets/img/2025-04-28-modnar/darksouls2.png"  class="img-fluid" %}

I will call it **KMP** for short. As implemented currently in CLRS-30,
the algorithm a bit less than 170 lines of code, dwarfing the 20 lines of
pseudocode of _Introduction to Algorithms_ (which I presume the reader is
familiar with). Yes, a lot of it is boilerplate, but I believe visualising
CLRS-30's implementation is worth the time.


The specification:
{% highlight python linenos %}
SPECS = types.MappingProxyType({
    # ... many algorithms omitted for brevity ...
    'kmp_matcher': {
        'string': (Stage.INPUT, Location.NODE, Type.MASK),
        'pos': (Stage.INPUT, Location.NODE, Type.SCALAR),
        'key': (Stage.INPUT, Location.NODE, Type.CATEGORICAL),
        'match': (Stage.OUTPUT, Location.NODE, Type.MASK_ONE),
        'pred_h': (Stage.HINT, Location.NODE, Type.POINTER),
        'pi': (Stage.HINT, Location.NODE, Type.POINTER),
        'is_reset': (Stage.HINT, Location.NODE, Type.MASK),
        'k': (Stage.HINT, Location.NODE, Type.MASK_ONE),
        'k_reset': (Stage.HINT, Location.GRAPH, Type.MASK),
        'q': (Stage.HINT, Location.NODE, Type.MASK_ONE),
        'q_reset': (Stage.HINT, Location.GRAPH, Type.MASK),
        's': (Stage.HINT, Location.NODE, Type.MASK_ONE),
        'i': (Stage.HINT, Location.NODE, Type.MASK_ONE),
        'phase': (Stage.HINT, Location.GRAPH, Type.MASK),
    }
    # ... many algorithms omitted for brevity ...
})
{% endhighlight %}
consist of a number of inputs and hints, which start in the following configuration:


<div class="l-page">
{% include figure.html path="assets/img/2025-04-28-modnar/kmp_0.svg"  class="img-fluid" %}
<div class="caption">
    Step 0 (Phase 0) - initialisation of phase 0
</div>
</div>

I will use green arrows for `MASK_ONE` indices like `i`, `s`, `k`, `q` and
purple for the output `match`.  The haystack nodes (gray squares) have
different colour from the needle (blue squares) -- this was the binary feature
of the `string` variable.  Pointers will be arrows _between_ squares (black for
`pred_h`; colour for `pi`). Note that a CLRS-30 convention/trick is to use
`pred_h` for input only as it never changes. Part of the pointers for `pi` (the
ones on the haystack) are dotted, to emphasise the fact, that they never
change. The `is_reset` node mask<d-footnote>It helps model the prefix function.
If this feature is set for the $i$-th position of $needle$, then for the
substring $needle_{:i}$ no suffix matches a prefix</d-footnote> will be
a little tickmark in bottom right corner if the feature is 1 and nothing
otherwise.  The `reset` graph masks are on the right of the diagram. I will not
visualise `phase` as it starts from 0 and changes only once, but will keep it
in the caption instead. Bear in mind that the model will need to predict the
phase as well.

You might notice there is no adjacency matrix here. Message passing occurs
between all pairs of nodes for this algorithm. 

KMP is a two phase algorithm. The aim of the first phase (Phase 0) of the algorithm
is to build the prefix function pointers `pi`. I will not go into explaining how
the algorithm works, but here is how CLRS-30 execution state of every step of
the first half of the algorithm.



<div class="l-page">
{% include figure.html path="assets/img/2025-04-28-modnar/kmp_1.svg"  class="img-fluid" %}
<div class="caption">
    Step 1 (Phase 0)
</div>
</div>

<div class="l-page">
{% include figure.html path="assets/img/2025-04-28-modnar/kmp_2.svg"  class="img-fluid" %}
<div class="caption">
    Step 2 (Phase 0)
</div>
</div>

<div class="l-page">
{% include figure.html path="assets/img/2025-04-28-modnar/kmp_3.svg"  class="img-fluid" %}
<div class="caption">
    Step 3 (Phase 0)
</div>
</div>

<div class="l-page">
{% include figure.html path="assets/img/2025-04-28-modnar/kmp_3p5.svg"  class="img-fluid" %}
<div class="caption">
    Step 4 (Phase 0)
</div>
</div>

You might notice several things:
- `k` did not move -- this was the case for this **toy** example and for a bit
  larger inputs (present in the training data) `k` may move.
- only 1 binary feature changed for step 4 -- this was again the case for this example.
- some of the `pi` pointers (the one on the haystack) are never touched -- this
  is a design choice of CLRS-30.<d-footnote>Otherwise one would need to figure out how to
  define the pointer array only for half of the nodes, which can make the code
  even messier.</d-footnote> Figuring out which pointers are subject to change
  is left to the GNN processor of the NAR.
- $i$ and $s$ did not move -- again this by design, they will in the second
  phase (Phase 1).

The next phase aims to use the built prefix function, i.e. the pointers `pi`,
in order to find the **start** position of the needle in the haystack. To
emphasise that in CLRS-30, this is not a different algorithm, step count
continues from 5.


<div class="l-page">
{% include figure.html path="assets/img/2025-04-28-modnar/kmp_5.svg"  class="img-fluid" %}
<div class="caption">
    Step 5 (Phase 1)
</div>
</div>

<div class="l-page">
{% include figure.html path="assets/img/2025-04-28-modnar/kmp_6.svg"  class="img-fluid" %}
<div class="caption">
    Step 6 (Phase 1)
</div>
</div>

<div class="l-page">
{% include figure.html path="assets/img/2025-04-28-modnar/kmp_7.svg"  class="img-fluid" %}
<div class="caption">
    Step 7 (Phase 1)
</div>
</div>

<div class="l-page">
{% include figure.html path="assets/img/2025-04-28-modnar/kmp_8.svg"  class="img-fluid" %}
<div class="caption">
    Step 8 (Phase 1)
</div>
</div>

<div class="l-page">
{% include figure.html path="assets/img/2025-04-28-modnar/kmp_9.svg"  class="img-fluid" %}
<div class="caption">
    Step 9 (Phase 1)
</div>
</div>

<div class="l-page">
{% include figure.html path="assets/img/2025-04-28-modnar/kmp_10.svg"  class="img-fluid" %}
<div class="caption">
    Step 10 (Phase 1)
</div>
</div>

<div class="l-page">
{% include figure.html path="assets/img/2025-04-28-modnar/kmp_11.svg"  class="img-fluid" %}
<div class="caption">
    Step 11 (Phase 1)
</div>
</div>


Allow me to draw your attention to the following observations:
- `k` again never moves -- this time it is by **algorithm** design (k is never
  used in the Phase 1, but we cannot delete variables)
- `k_reset` never changes -- again by design
- `pi` pointers never change -- again by design
- `is_reset` (the tick in the boxes) never changes -- again by design

It should also be fairly obvious that Phase 1 performs a different type of work --
moving indices around, and following previously computed pointers, instead of
changing them. At each step it also uses a different set variables as input
(e.g. the characters under indices `i` and `q` dictate how the `q` pointer
moves, not the ones under `k` and `q`) and changes a different set of variables
at output (pointers remain untouched). While the lack of persistency in
NAR<d-cite key="jain2024neural"></d-cite> (i.e. recalling information from many
steps ago) could be one possible explanation, I propose an alternative
hypothesis.


<div class="takeaway">
    It is currently not possible to switch GNN behaviour during a NAR execution
    based only on a single hint feature.
</div>

<!-- Note that the above does not contradict the generalist NAR paper <d-cite -->
<!-- key="ibarz2022generalist"></d-cite>, since (to my best understanding) in the -->
<!-- generalist paper -->

## Fighting KMP

